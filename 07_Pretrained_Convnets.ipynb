{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manedk/AIArchitect/blob/main/07_Pretrained_Convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYJVxd-kfBZr"
      },
      "source": [
        "# 07 Pretrained Convnets\n",
        "**Adapted from Deep Learning with Pyton by Francois Chollet**\n",
        "\n",
        "https://github.com/fchollet/deep-learning-with-python-notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yx_yE_cfBZr"
      },
      "source": [
        "## Using a Pre-trained Convnet\n",
        "\n",
        "A pre-trained network is simply a saved network previously trained on a large dataset, typically on a large-scale image classification task. For instance, one might train a network on ImageNet (where classes are mostly animals and everyday objects) and then re-purpose this trained network for something as remote as identifying furniture items in images.\n",
        "\n",
        "We will consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1000 different classes).\n",
        "ImageNet contains many animal classes, including different species of cats and dogs, and we can thus expect to perform very well on our cat vs. dog classification problem.\n",
        "\n",
        "We will use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014, a simple and widely used convnet architecture for ImageNet.\n",
        "\n",
        "There are two ways to leverage a pre-trained network: *feature extraction* and *fine-tuning*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Cc2SqSfBZs"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "\n",
        "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples.\n",
        "These features are then run through a new classifier, which is trained from scratch.\n",
        "\n",
        "Convnets used for image classification comprise two parts: they start with a series of pooling and convolution layers, and they end with a densely-connected classifier. The first part is called the \"convolutional base\" of the model. In the case of convnets, \"feature extraction\" will simply consist of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output.\n",
        "\n",
        "![swapping FC classifiers](https://s3.amazonaws.com/book.keras.io/img/ch5/swapping_fc_classifier.png)\n",
        "\n",
        "The representations learned by the convolutional base are likely to be more generic.\n",
        "\n",
        "The level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual\n",
        "edges, colors, and textures), while layers higher-up extract more abstract concepts (such as \"cat ear\" or \"dog eye\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTmB_2w6fBZs"
      },
      "source": [
        "The VGG16 model comes pre-packaged with Keras. You can import it from the `keras.applications` module. Here's the list of image classification models (all pre-trained on the ImageNet dataset) that are available as part of `keras.applications`:\n",
        "\n",
        "* Xception\n",
        "* InceptionV3\n",
        "* ResNet50\n",
        "* VGG16\n",
        "* VGG19\n",
        "* MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpDy6ldofBZt"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB2F3DdHfBZv"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHYS_XeLfBZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806bf803-8416-4dcd-ab8f-c72267d658b4"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf_AFNatfBZ0"
      },
      "source": [
        "We passed three arguments to the constructor:\n",
        "\n",
        "* `weights`, to specify which weight checkpoint to initialize the model from.\n",
        "* `include_top`, which refers to including or not the densely-connected classifier on top of the network. By default, this\n",
        "densely-connected classifier would correspond to the 1000 classes from ImageNet. Since we intend to use our own densely-connected\n",
        "classifier (with only two classes, cat and dog), we don't need to include it.\n",
        "* `input_shape`, the shape of the image tensors that we will feed to the network. This argument is purely optional: if we don't pass it,\n",
        "then the network will be able to process inputs of any size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7FVrdeEfBZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e94aa43-99d8-4b2f-97a8-30627856a197"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rauXSXp1fBZ2"
      },
      "source": [
        "The final feature map has shape `(4, 4, 512)`.\n",
        "\n",
        "At this point, there are two ways we could proceed:\n",
        "\n",
        "* Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a standalone densely-connected classifier. This solution is very fast and cheap to run but would not allow us to leverage data augmentation at all.\n",
        "* Extending the model we have (`conv_base`) by adding `Dense` layers on top, and running the whole thing end-to-end on the input data. This allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. This technique is expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MnnWNtXnRm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90379b4a-3a2b-48f6-94b1-3eb84653b7a3"
      },
      "source": [
        "# For using it from google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auOlLnKXfBZ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5db23e22-3088-4384-835b-0efca9eb51ee"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#base_dir = '/home/mabeigier/Downloads/cats_and_dogs_small'\n",
        "base_dir = 'drive/My Drive/Colab_Notebooks/AI_Full_Path_v2/datasets/cats_and_dogs_small'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size = (150, 150),\n",
        "        batch_size = batch_size,\n",
        "        class_mode = 'binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, 2000)\n",
        "print(\"Train features: \", train_features.shape)\n",
        "validation_features, validation_labels  = extract_features(validation_dir, 1000)\n",
        "print(\"Validation features: \", validation_features.shape)\n",
        "test_features, test_labels = extract_features(test_dir, 1000)\n",
        "print(\"Test features: \", test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Train features:  (2000, 4, 4, 512)\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7a5dba751cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train features: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation features: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test features: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzlInC4nfBZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "27632887-7777-470c-a0f4-d30c262c0327"
      },
      "source": [
        "# Flatten\n",
        "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
        "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
        "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8c3519f169f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAYbFvkJfBZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387c2e0a-1c03-488a-d959-900e8ea41a12"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "import tensorflow\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=tensorflow.keras.optimizers.RMSprop(lr=2e-5),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['acc'])\n",
        "\n",
        "#history = model.fit(train_features, train_labels,\n",
        "#                   epochs=30,\n",
        "#                   batch_size=20,\n",
        "#                   validation_data=(validation_features, validation_labels))\n",
        "\n",
        "history = model.fit(train_features, train_labels,\n",
        "                   epochs=30,\n",
        "                   batch_size=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 3s 5ms/step - loss: 0.5963 - acc: 0.6770\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.4364 - acc: 0.7900\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.3625 - acc: 0.8545\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.3194 - acc: 0.8725\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2887 - acc: 0.8840\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2673 - acc: 0.8950\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2460 - acc: 0.9075\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2366 - acc: 0.9135\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2194 - acc: 0.9210\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.2051 - acc: 0.9225\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1946 - acc: 0.9285\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1891 - acc: 0.9340\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1794 - acc: 0.9325\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1691 - acc: 0.9340\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1658 - acc: 0.9375\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1532 - acc: 0.9460\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1519 - acc: 0.9485\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1444 - acc: 0.9505\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1384 - acc: 0.9515\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1332 - acc: 0.9565\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1270 - acc: 0.9580\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.1248 - acc: 0.9570\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.1152 - acc: 0.9655\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1128 - acc: 0.9600\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1081 - acc: 0.9610\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1070 - acc: 0.9615\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.1006 - acc: 0.9655\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.0910 - acc: 0.9750\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0887 - acc: 0.9720\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.0847 - acc: 0.9730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAmBg1F4fBZ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "de3c7572-74c2-4f33-eae4-78a138e02019"
      },
      "source": [
        "acc = history.history['acc']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c3e59c224f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_acc' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn3dokUAfBZ-"
      },
      "source": [
        "The second technique for doing feature extraction, which is much slower and more expensive, but which allows\n",
        "us to leverage data augmentation during training: extending the `conv_base` model and running it end-to-end on the inputs. Note that this technique is in fact so expensive that you should only attempt it if you have access to a GPU: it is absolutely intractable on CPU. If you cannot run your code on GPU, then the previous technique is the way to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuE7CBwufBZ_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HIY4Zt_fBaC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}